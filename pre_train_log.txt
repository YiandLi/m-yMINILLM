QWen size: 464.0M parameters
final vocab size: 151680 ; use map dtye <class 'numpy.uint32'>
QWen size: 464.0M parameters
final vocab size: 151680 ; use map dtye <class 'numpy.uint32'>
Total data set length: 1990025
Using Iterable Dataset
Train Dataset IterableDataset({
    features: Unknown,
    n_shards: 1
})
Total data set length: 1990025
Using Iterable Dataset
Train Dataset IterableDataset({
    features: Unknown,
    n_shards: 1
})
/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
***** Running training *****
  Num examples = 31,840,448
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 497,507
  Number of trainable parameters = 463,987,712
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
Too many dataloader workers: 12 (max is dataset.n_shards=1). Stopping 11 dataloader workers.
wandb: Currently logged in as: yil. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /root/myMiniLLM/wandb/run-20240412_083806-6oiba0s7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-oath-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/yil/huggingface
wandb: üöÄ View run at https://wandb.ai/yil/huggingface/runs/6oiba0s7
  0%|                                                                                                                                                                                              | 0/497507 [00:00<?, ?it/s]Too many dataloader workers: 12 (max is dataset.n_shards=1). Stopping 11 dataloader workers.
The following columns in the training set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/data_loader.py", line 611, in _fetch_batches
    batch = concatenate(batches, dim=0)
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/utils/operations.py", line 626, in concatenate
    return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/utils/operations.py", line 626, in <dictcomp>
    return type(data[0])({k: concatenate([d[k] for d in data], dim=dim) for k in data[0].keys()})
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/utils/operations.py", line 629, in concatenate
    return torch.cat(data, dim=dim)
RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 435 but got size 445 for tensor number 1 in the list.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/root/myMiniLLM/train.py", line 127, in <module>
    trainer.train(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 1780, in train
    return inner_training_loop(
  File "/root/miniconda3/lib/python3.10/site-packages/transformers/trainer.py", line 2085, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/data_loader.py", line 655, in __iter__
    next_batch, next_batch_info = self._fetch_batches(main_iterator)
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/data_loader.py", line 613, in _fetch_batches
    raise RuntimeError(
RuntimeError: You can't use batches of different size with `dispatch_batches=True` or when using an `IterableDataset`.either pass `dispatch_batches=False` and have each process fetch its own batch  or pass `split_batches=True`. By doing so, the main process will fetch a full batch and slice it into `num_processes` batches for each process.
wandb: üöÄ View run vague-oath-22 at: https://wandb.ai/yil/huggingface/runs/6oiba0s7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/yil/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240412_083806-6oiba0s7/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1207 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1206) of binary: /root/miniconda3/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1066, in launch_command
    multi_gpu_launcher(args)
  File "/root/miniconda3/lib/python3.10/site-packages/accelerate/commands/launch.py", line 711, in multi_gpu_launcher
    distrib_run.run(args)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-12_08:38:36
  host      : autodl-container-6f4147b966-9f535de0